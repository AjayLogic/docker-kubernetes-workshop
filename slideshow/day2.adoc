:footer_copyright: Copyright Â©2015 Red Hat, Inc.
:imagesdir: images/
:speaker: Christian Posta
:speaker-title: Principal Middleware Architect
:speaker-email: christian@redhat.com
:speaker-blog: http://blog.christianposta.com
:speaker-twitter: http://twitter.com/christianposta[@christianposta]
:talk-speaker: {speaker}
:talk-name: Intro: Docker and Kubernetes training - Day 2
:talk-date: 10/19/2015

[#cover,data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,width="600px",left="0px",top="200px"]
{talk-name}

[#cover-h2,width="800px",left="0px",top="450px"]
{speaker} +
{talk-date}

// ************** who - christian ********
[#who]
== Who

[.noredheader,cols="30,70"]
|===
| image:ceposta.png[width="90%",height="100%"]
| {speaker-title}

Blog: {speaker-blog}

Twitter: {speaker-twitter}

Email: {speaker-email} |
|===

* Committer on Apache ActiveMQ, Apache Camel, Fabric8
* Technology evangelist, recovering consultant
* Spent a lot of time working with one of the largest Microservices, web-scale, unicorn companies
* Frequent blogger and speaker about open-source, cloud, microservices

// ************** Agenda  ********
[#agenda]
== Agenda

* Intro / Prep Environments
* Day 1: Docker Deep Dive
* *Day 2: Kubernetes Deep Dive*
* Day 3: Advanced Kubernetes: Concepts, Management, Middleware
* Day 4: Advanced Kubernetes: CI/CD, open discussions


// ************** transition page ************
[#transition1, data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,width="600px",left="0px",top="400px"]
*Quick Recap*

// ************** Recap ********
[#recap1]
== Recap Docker

* *Linux* containers
* Docker API
* Images
* Containers
* Registry

// ************** Recap ********
[#recap2]
== Why  Docker matters

* Application distribution
* Dependency management
* Application density
* Reduced management overhead in terms of VMs
* On premise, hybrid, public cloud


// ************** Recap ********
[#recap3]
== Recap Docker

* Containers run on *single* Docker host
* Containers are *ephemeral*
* Nothing watchdogs the containers
* Containers can have external persistence
* Containers do not contain
* Operating system *matters*

// ************** Recap ********
[#recap3]
== Why you win with Docker-based solutions

* Immutable infrastructure
* DevOps
* CI/CD
* *Who cares:* give me a platform to move faster!!!


// ************** transition page **************************************************************************************
[#transition2, data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,left="0px",top="350px",width="2000px"]
*Local environment setup*


// ************** Environment setup ***********
[#prep]
== Set up kubernetes

* Lab prerequisites in place!
* Verify you have Oracle VirtualBox 4.3.x
* Install Kubernetes link:http://kubernetes.io/v1.0/docs/getting-started-guides/vagrant.html[http://kubernetes.io/v1.0/docs/getting-started-guides/vagrant.html]
* Windows: need to have HVT
* Understand the architecture
* Smoke test



// ************** Environment setup ***********
[#prep-output]
== Final output

    Waiting for each minion to be registered with cloud provider
    Validating we can run kubectl commands.
    NAME      READY     STATUS    RESTARTS   AGE
    Connection to 127.0.0.1 closed.

    Kubernetes cluster is running.  The master is running at:

      https://10.245.1.2

    The user name and password to use is located in ~/.kubernetes_vagrant_auth.

    calling validate-cluster
    Found 1 nodes.
            NAME         LABELS                              STATUS
         1  10.245.1.3   kubernetes.io/hostname=10.245.1.3   Ready
    Validate output:
    NAME                 STATUS    MESSAGE              ERROR
    controller-manager   Healthy   ok                   nil
    scheduler            Healthy   ok                   nil
    etcd-0               Healthy   {"health": "true"}   nil
    Cluster validation succeeded
    Done, listing cluster services:

    Kubernetes master is running at https://10.245.1.2
    KubeDNS is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/kube-dns
    KubeUI is running at https://10.245.1.2/api/v1/proxy/namespaces/kube-system/services/kube-ui


// ************** Environment setup ***********
[#simpelarch]
== Simple kubernetes architecture

[#block,width="200px",top="150px",left="75px"]
image:day2/kube-diagram.png[width="170%",height="170%"]

// ************** Environment setup ***********
[#fullarch]
== Overall Kubernetes

[#block,width="200px",top="75px",left="50px"]
image:day2/kubernetes-platform.png[width="100%",height="100%"]



// ************** transition page **************************************************************************************
[#transition-kube, data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,left="0px",top="250px",width="2000px"]
*Kubernetes*


[#containterizeit]
== Containerize all the things!

*Everything* at Google runs in containers!!

* Gmail, search, maps
* 2 billion containers *a week*
* GCE? VMs in containers...

[#block,width="200px",left="25px",top="300px"]
image:day2/all-containers.jpg[]

// ************** Kubernetes intro ***********
[#whatisit1]
== Kube what?

* "helmsman of a ship"
* Containers @ Google
* Borg link:http://www.infoq.com/news/2015/04/google-borg[http://www.infoq.com/news/2015/04/google-borg]
* Open source 6/2014
* GKE...
* Red Hat a top contributor
* 100% written in *golang*
* Do we need this?

[#block,width="200px",left="200px",top="300px"]
image:kubernetes.png[]




// ************** Kubernetes intro ***********
[#whatisit2]
== What is Kubernetes

* Different way to look at managing instances: scale
* Design for failure
* Efficient / Lean/ Simple
* Portability
* Extensible


// ************** Kubernetes intro ***********
[#whatisit3]
== What is Kubernetes

* How to place containers on a cluster
* Smart placement
* How to interact with a system that does placement
* Different than configuration management
** Immutable infrastructure principles
* What to do when containers fail?
* *Containers will fail*
* Cluster security authZ/authN
* Scaling
* Grouping/Aggregates


// ************** Kubernetes intro ***********
[#whyisitimportant]
== Why is it important

* Managing containers by hand is harder that VMS: won't scale
* Automate the boilerplate stuff
* Runbooks -> Scripts -> Config management -> Scale
* Decouple application from machine!
* Applications run on "resources"
* Kubernetes manages this interaction of applications and resources
* *Manage applications, not machines!*
* What about *legacy apps?*


// ************** Kubernetes intro ***********
[#coreconcets]
== Kubernetes core concepts

* Simplicity, Simplicity, Simplicity
* *Pods*
* *Labels* / *Selectors*
* *Replication Controllers*
* *Services*
* API -- link:http://kubernetes.io/third_party/swagger-ui/[http://kubernetes.io/third_party/swagger-ui/]

[#block,width="200px",left="250px",top="400px"]
image:day2/kube-pods.png[]



// ************** Kubernetes intro ***********
[#controlplane]
== Reconciliation of end state

[#block,width="200px",left="50px",top="170px"]
image:day2/make-it-so.png[height="200%",width="200%"]




// ************** Kubernetes intro ***********
[#controlplane]
== Kubernetes control plane

* etcd
* API Server
* Scheduler
* Controller manager

[#block,width="200px",left="50px",top="300px"]
image:day2/kube-control-plane.png[height="150%",width="150%"]


// ************** Kubernetes intro ***********
[#etcd]
== etcd

* Open source project started at CoreOS
* Distributed database
* CAP Theorem? == CP
* Raft algorithm/protocol
* watchable
* etcd provides HA datastore


[#block,width="200px",left="250px",top="250px"]
image:day2/etcd.png[height="75%",width="75%"]


// ************** Kubernetes intro ***********
[#nodes]
== Kubernetes nodes

* Nodes are VMs / physical hosts
* Nodes need connectivity between them
* Ideally same network/data center/availability zone


[#block,width="200px",left="50px",top="250px"]
image:day2/node-connectivity.png[height="150%",width="150%"]


// ************** Kubernetes intro ***********
[#nodes1]
== Kubernetes nodes

* Kubelet
* kube-proxy
* Docker

[#block,width="200px",left="50px",top="250px"]
image:day2/kube-control-plane-nodes.png[height="150%",width="150%"]


// ************** Kubernetes intro ***********
[#cluster-addons]
== Cluster add-ons

* Monitoring
* DNS
* UI
* Logging



// ************** Kubernetes intro ***********
[#quick-demo]
== Quick Demo!

[#block,width="200px",left="50px",top="50px"]
image:day2/demo.jpg[height="100%",width="100%"]











// ************** transition page **************************************************************************************
[#deepdive, data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,left="0px",top="350px",width="2000px"]
*Kubernetes Deep Dive*



// ************** Kubernetes deep dive ***********
[#core-concepts]
== Kubernetes core concepts

* *Pods*
* *Labels* / *Selectors*
* *Replication Controllers*
* *Services*


// ************** Kubernetes deep dive ***********
[#deep-pods]
== Kubernetes Pods

* A pod is one or more docker containers
* Ensures collocation / shared fate
* Pods are scheduled and do not move nodes
* Docker containers share resources within the pod
** Volumes
** Network / IP
** Port space
** CPU / Mem allocations
* Pod health probes
** Readiness
** Liveness


// ************** Kubernetes deep dive ***********
[#deep-pods-image]
== Kubernetes Pods

[#block,width="400px",left="50px",top="100px"]
image:day2/deep-pod.png[height="175%",width="175%"]


// ************** Kubernetes deep dive ***********
[#pod-creation]
== Kubernetes Pods

[#block,width="400px",left="50px",top="100px"]
image:day2/pod-creation.png[height="175%",width="175%"]

// ************** Kubernetes deep dive ***********
[#deep-pods-probes]
== Pod probes

* Out of the box
** Readiness
** Liveness
* Probe types
** ExecAction
** TCPSocketAction
** HTTPGetAction
* Results
** Success
** Failure
** Unknown


// ************** Kubernetes deep dive ***********
[#deep-pods-restart-policy]
== Pod restart policies

* Pods restarted on single node
* Only replication controller reschedules
* RestartPolicy
** Always (default)
** OnFailure
** Never
* Applies to all containers in the pod
* For replication controllers, only _Always_ applicable

// ************** Kubernetes deep dive ***********
[#deep-pods-restart-policy2]
== Pod restart policies

Some Examples:

* Pod is Running, 1 container, container exits success
** Always: restart container, pod stays Running
** OnFailure: pod becomes Succeeded
** Never: pod becomes Succeeded
* Pod is Running, 1 container, container exits failure
** Always: restart container, pod stays Running
** OnFailure: restart container, pod stays Running
** Never: pod becomes Failed
* Pod is Running, 2 containers, container 1 exits failure
** Always: restart container, pod stays Running
** OnFailure: restart container, pod stays Running
** Never: pod stays Running

When container 2 exits...

** Always: restart container, pod stays Running
** OnFailure: restart container, pod stays Running
** Never: pod becomes Failed

// ************** Kubernetes deep dive ***********
[#deep-pods-termination]
== Termination messages

* Help debug problems by putting messages into "termination log"
* default location `/dev/termination-log`
* Quick demo?


// ************** Kubernetes deep dive ***********
[#first-pod]
== Your first pod

Let's take a look at the Kubernetes resource file that we will use and evolve for the purposes of this first
hands-on experience:

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: influxdb
  name: influxdb
spec:
  containers:
    - image: docker.io/tutum/influxdb:latest
      name: influxdb
      ports:
        - containerPort: 8083
          name: admin
          protocol: TCP
        - containerPort: 8086
          name: http
          protocol: TCP
```



// ************** Kubernetes deep dive ***********
[#deploy-first-pod]
== Deploy the influxdb pod

You can either copy/paste that locally, or download the pod resource file like this:

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/influxdb-simple.yaml -O influxdb-simple.yaml
```

Once you have the file locally downloaded, run the kubectl command (`./cluster/kubectl.sh`) like this:

```
kubectl create -f influxdb-simple.yaml
```

Wait for the Docker image to download and the pod to start. Can watch it like this:

```
kubectl get pod --watch
```

Can watch the docker events in another window like this (to keep track of what's happening on the docker side) -- note, you'll need to be logged into `minion-1` directly to see the docker events:

```
docker events
```

// ************** Kubernetes deep dive ***********
[#inspect-first-pod]
== Interact with your first Kubernetes pod

Now that we have our first pod up, let's see where it's deployed:

```
kubectl describe pod/influxdb
```

Note where it's deployed (Node field).

    ceposta@postamac(kubernetes (master)) $ kubectl describe pod/influxdb
    Name:                           influxdb
    Namespace:                      demo-pods
    Image(s):                       docker.io/tutum/influxdb:latest
    Node:                           10.245.1.3/10.245.1.3
    Labels:                         name=influxdb
    Status:                         Running
    Reason:
    Message:
    IP:                             10.246.1.7
    Replication Controllers:        <none>
    Containers:
      influxdb:
        Image:              docker.io/tutum/influxdb:latest
        State:              Running
          Started:          Sun, 18 Oct 2015 17:09:16 -0700
        Ready:              True
        Restart Count:      0
    Conditions:
      Type          Status
      Ready         True
    Events:
    <truncated>


// ************** Kubernetes deep dive ***********
[#query-our-first-pod]
== Interact with your first Kubernetes pod

Let's login to our minion-1 machine and interact with the pod. We found out which machine the pod was running on in the last slide.

```
vagrant ssh minion-1
```

Let's try ping the pod to see if we can reach it from the minion (we could have also logged into the `master` and run the same commands; the assumption kubernetes makes is a flat network reachable from anywhere in the cluster. More on the kubrentes network model in Day 3).

```
ping 10.246.1.7
```

Note, you should use the IP address of the pod from the above `kubectl describe` command.


    [vagrant@kubernetes-master ~]$ ping 10.246.1.7
    PING 10.246.1.7 (10.246.1.7) 56(84) bytes of data.
    64 bytes from 10.246.1.7: icmp_seq=1 ttl=63 time=2.43 ms
    64 bytes from 10.246.1.7: icmp_seq=2 ttl=63 time=8.41 ms
    64 bytes from 10.246.1.7: icmp_seq=3 ttl=63 time=1.27 ms
    64 bytes from 10.246.1.7: icmp_seq=4 ttl=63 time=2.23 ms
    ^C
    --- 10.246.1.7 ping statistics ---
    4 packets transmitted, 4 received, 0% packet loss, time 3004ms
    rtt min/avg/max/mdev = 1.271/3.587/8.411/2.819 ms


// ************** Kubernetes deep dive ***********
[#query-our-first-pod2]
== Interact with your first Kubernetes pod

Now, let's issue a command to that IP address. Note, these commands must be run from either `master` or `minion-1`

```
export INFLUX_NODE=10.246.1.7
curl -G "http://$INFLUX_NODE:8086/query" --data-urlencode "q=CREATE DATABASE kubernetes"
curl -X POST "http://$INFLUX_NODE:8086/write?db=kubernetes" -d 'pod,host=node0 count=10'
curl -G "http://$INFLUX_NODE:8086/query?pretty=true" --data-urlencode "db=kubernetes" --data-urlencode "q=SELECT SUM(count) FROM pod"
```

We add a new database to the influxdb index, then we add some data, and finally we query it. You should see '10' as the result.


// ************** Kubernetes deep dive ***********
[#is-pod-healthy]
== Adding readiness and liveness checks

At this point, we've deployed our first application on Kubernetes, but it doesn't do too much. Earlier we discussed the different probe options we have for determining whether a pod is healthy or ready for action. Let's add some of that to our pod. The pod YAML resource looks like this:

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: influxdb
  name: influxdb
spec:
  containers:
    - image: docker.io/tutum/influxdb:latest
      name: influxdb
      readinessProbe:
        httpGet:
          path: /ping
          port: 8086
        initialDelaySeconds: 5
        timeoutSeconds: 1
      livenessProbe:
        httpGet:
          path: /ping
          port: 8086
        initialDelaySeconds: 15
        timeoutSeconds: 1
      ports:
        - containerPort: 8083
          name: admin
          protocol: TCP
        - containerPort: 8086
          name: http
          protocol: TCP
```

// ************** Kubernetes deep dive ***********
[#is-pod-healthy2]
== Adding readiness and liveness checks

First, let's stop and remove our previous `influxdb` pod since we'll be re-doing it this time with health-checking:

```
kubectl stop pod/influxdb
```

Curl the pod resource as we did before.

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/influxdb-readiness.yaml -O influxdb-rediness.yaml
```

Use the `kubectl` command line again to create our pod:

```
kubectl create -f influxdb-readiness.yaml
```

// ************** Kubernetes deep dive ***********
[#is-pod-healthy3]
== Adding readiness and liveness checks

Now that we've deployed our liveness and readiness checks, our Pod is not automatically monitored by kubernetes. If there is ever an issue with it, Kubernetes will kill the pod and depending on its *restartPolicy* will take action.


// ************** Kubernetes deep dive ***********
[#pod-resources]
== Adding resource constraints to our pods

We have readiness and liveness checks, but we may also want to constrain what the `influxdb` pod(s) do in terms of isolation and CPU/Memory usage. We don't want a single pod taking over all of the underlying resources and starving other processes.

Let's alter our pod yaml resource descriptor to add in resource constraints:

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: influxdb
  name: influxdb
spec:
  containers:
    - image: docker.io/tutum/influxdb:latest
      name: influxdb
      resources:
        limits:
          cpu: "500m"
          memory: "128Mi"
      readinessProbe:
        httpGet:
          path: /ping
          port: 8086
        initialDelaySeconds: 5
        timeoutSeconds: 1
      livenessProbe:
        httpGet:
          path: /ping
          port: 8086
        initialDelaySeconds: 15
        timeoutSeconds: 1
      ports:
        - containerPort: 8083
          name: admin
          protocol: TCP
        - containerPort: 8086
          name: http
          protocol: TCP
```

// ************** Kubernetes deep dive ***********
[#pod-resources]
== Adding resource constraints to our pods

Let's stop our previous pod, download the new manifest and run it:

```
kubectl stop pod/influxdb
```

Curl the pod resource as we did before.

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/influxdb-resource-limits.yaml -O influxdb-resource-limits.yaml
```


Use the `kubectl` command line again to create our pod:

```
kubectl create -f influxdb-resource-limits.yaml
```

Now our pod is allocated specific limits so we can sleep at night knowing we have resource isolation.

// ************** Kubernetes deep dive ***********
[#pods-volumes]
== Stateful containers

Whenever we add new databases to our `influxdb` and then stop the pod, we find that the data in the `influxdb` is not persisted. Let's add a way to persist that information.


Kubernetes offers a few persistence mechanisms out of the box:

* emtpyDir
* hostpath
* gcePersistentDisk
* awsElasticBlockStorage
* nfs
* glusterfs
* gitRepo

// ************** Kubernetes deep dive ***********
[#pods-volumes-hostpath]
== Stateful containers

In tomorrow's demo, we'll show how to use a shared-storage mechanism in Google Compute Engine to create stateful pods. But for now, we'll just map to a location on the Node/host. Note this has implications about pod rescheduling etc that will discussed in the class.

Let's create the Kubernetes Persistent Volume

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/local-persistent-volume.yaml | kubectl create -f -
```

Now we're going to create a "claim" for persistent volume space (details explained in class):

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/local-pvc.yaml | kubectl create -f -
```

Make sure our influxdb pod is stopped

```
kubectl stop pod influxdb
```

And now deploy our new pod that uses the PersistentVolume:

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/influxdb-local-pv.yaml | kubectl create -f -
```

// ************** Kubernetes deep dive ***********
[#pods-volumes-query-influx]
== Stateful containers

Now let's figure out what IP address the new influxd pod lives:

```
kubectl describe pod/influxdb
```

And now let's query/interact with influxdb

```
export INFLUX_NODE=10.246.1.7
curl -G "http://$INFLUX_NODE:8086/query" --data-urlencode "q=CREATE DATABASE kubernetes"
curl -X POST "http://$INFLUX_NODE:8086/write?db=kubernetes" -d 'pod,host=node0 count=10'
curl -X POST "http://$INFLUX_NODE:8086/write?db=kubernetes" -d 'pod,host=node1 count=9'
curl -X POST "http://$INFLUX_NODE:8086/write?db=kubernetes" -d 'pod,host=node2 count=12'
curl -G "http://$INFLUX_NODE:8086/query?pretty=true" --data-urlencode "db=kubernetes" --data-urlencode "q=SELECT SUM(count) FROM pod"
```


// ************** Kubernetes deep dive ***********
[#pods-volumes-query-influx-restart]
== Stateful containers

Now let's kill the pod and restart it. We should see the same databases and data in the influxbd even after we restart it:

```
kubectl stop pod/influxdb
```

Restart it:

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/hands-on-pods/influxdb-local-pv.yaml | kubectl create -f -
```

Query it:

```
curl -G "http://$INFLUX_NODE:8086/query?pretty=true" --data-urlencode "db=kubernetes" --data-urlencode "q=SELECT SUM(count) FROM pod"
```

// ************** Kubernetes deep dive ***********
[#pods-in-production]
== Pods in production

* Use PersistentVolumes for stateful pods/containers
* Use Kubernetes *secrets* to distribute credentials
* Leverage readiness and liveness probes
* Consider resources (CPU/Mem) limites/quotas
* Use termination messages


// ************** transition page **************************************************************************************
[#grouping, data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,left="0px",top="350px",width="2000px"]
*Grouping, organizing your cluster*


// ************** Labels ********
[#grouping-concepts]
== Grouping

* Decouple application dimensions from infrastructure deployments
** Tiers
** Release Tracks
** Grouping of microservices
* Kubernetes clusters are dynamic
* Predicated on failure
* Need a way to identify groups of services
* Be able to add/remove from group on failures
* Needs to be flexible
* Queryable
* Kubernetes Labels


// ************** Labels ********
[#what-are-labels]
== What are Kubernetes Labels?

* Arbitrary metadata
* Attach to any API objects
** eg, Pods, ReplicationControllers, Endpoints, etc
* Simple *key=value* assignments
* Can be queried with *selectors*


// ************** Labels ********
[#label-examples]
== Examples

* release=stable, release=canary
* environment=dev, environment=qa, environment=prod
* tier=frontend, tier=backend, tier=middleware
* partition=customerA, partition=customerB, etc
* used to group âlikeâ objects â no expectation of uniqueness


// ************** Labels ********
[#key-values]
== Keys and Values

* The structure of a key:
** <prefix>/<name>
** example key *`org.apache.hadoop/partition`*
* Prefix can be up to `253` chars
* Name can be up to `63` chars
* Values can be ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.)
* Values can be up to `63` chars

// ************** Labels ***********
[#examplelabels]
== Example labels

[#block,width="200px",top="70px",left="25px"]
image:day2/label-pods.png[width="150%",height="150%"]


// ************** Labels ********
[#selecors]
== Selectors for grouping

Labels are queryable metadata; selectors can do the queries:

* Equality based
** environment *=* production
** tier *!=* frontend
** combinations: "tier != frontend, version = 1.0.0"
* Set based
** environment *in* (production, pre-production)
** tier *notin* (frontend, backend)
** partition


// ************** Labels ***********
[#exampleselectors]
== Example Selectors

[#block,width="200px",top="70px",left="25px"]
image:day2/pod-selectors.png[width="150%",height="150%"]

// ************** Labels ********
[#annotations]
== Annotations

* Attaching useful metadata to objects; not queryable
* Data that you want to know to build context; easy to have it close
** Author information
** build date/timestamp
** links to SCM
** PR numbers/gerrit patchsets
* Annotations are *not* queryable *use labels for that)
* Can build up tooling around annotations
* link:https://oss.sonatype.org/content/repositories/public/io/fabric8/quickstarts/quickstart-karaf-camel-log/2.2.49-SNAPSHOT/quickstart-karaf-camel-log-2.2.49-20151014.171515-12-kubernetes.json[Examples]


// ************** Labels ********
[#what-should-you]
== What should you label and annotate?

Everything.



// ************** transition page **************************************************************************************
[#transition-repl, data-background-image="revealjs-redhat/image/1156524-bg_redhat.png" data-background-color="#cc0000"]
== {blank-space}

[#block,width="200px",left="70px",top="0px"]
image::{revealjs_cover_image}[]

[#cover-h1,left="0px",top="250px",width="2000px"]
*Scaling and ensuring cluster invariants*

// ************** Scaling pods ***********
[#repl-makeitso]
== Replication controllers

[#block,width="200px",left="50px",top="170px"]
image:day2/make-it-so.png[height="200%",width="200%"]

// ************** Scaling pods ***********
[#replcontrollers-intro]
== Replication controllers

* Can configure the number of *replicas* of a pod
* Will be scheduled across applicable nodes
* Replication controllers can change *replica* value to scale up/down
* Which pods are scaled depends on RC selector
* Labels and selectors are the backbone of grouping


// ************** Scaling pods ***********
[#replcontrollers-intro]
== Replication controllers

* Can even do complicated things with labels/RCs
* Take a pod out of cluster by changing its label
* Can have multiple RCs so no SPOF
* If a pod is unhealth (health prob), RC can kill and restart

// ************** Scaling pods ***********
[#replcontrollers-eg]
== Example Replication controller

```
apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    app: inspector
    track: stable
  name: inspector
spec:
  replicas: 1
  selector:
    app: inspector
    track: stable
  template:
    metadata:
      labels:
        app: inspector
        track: stable
    spec:
      containers:
      - image: b.gcr.io/kuar/inspector:1.0.0
        imagePullPolicy: IfNotPresent
        name: inspector
        resources: {}
        terminationMessagePath: /dev/termination-log
      dnsPolicy: ClusterFirst
      restartPolicy: Always
status:
  observedGeneration: 1
  replicas: 1
```

// ************** Scaling pods ***********
[#replcontrollers-eg-locally]
== Example Replication controller

On your cluster, run the following command:

```
curl -s -L https://raw.githubusercontent.com/christian-posta/docker-kubernetes-workshop/master/demos/replication-controllers/inspector-rc.yaml | kubectl create -f -
```

Then do:

```
kubectl get rc
```

You should see:

    CONTROLLER   CONTAINER(S)   IMAGE(S)                        SELECTOR                     REPLICAS
    inspector    inspector      b.gcr.io/kuar/inspector:1.0.0   app=inspector,track=stable   1


// ************** Scaling pods ***********
[#replcontrollers-eg-scale]
== Example scale up

We want to change the value of the `*replicas*` for our `inspector` replication controller to scale up:

```
kubectl scale rc inspector --replicas=5
```

If successful, you should see:

    scaled

Now do a "get pods" to see

```
kubectl get pod
```

    NAME              READY     STATUS    RESTARTS   AGE
    inspector-3sh6q   1/1       Running   0          13m
    inspector-5djgp   1/1       Running   0          13m
    inspector-9b68m   1/1       Running   0          13m
    inspector-ohaod   1/1       Running   0          13m
    inspector-r5bqo   1/1       Running   0          19m

// ************** Scaling pods ***********
[#replcontrollers-eg-scale-down]
== Example scale down

Let's scale the inspector app down to two pods:

```
kubectl scale rc inspector --replicas=2
```

Now when you get the pods, there should just be two. Kubernetes decides which ones to kill:

    NAME              READY     STATUS    RESTARTS   AGE
    inspector-ohaod   1/1       Running   0          15m
    inspector-r5bqo   1/1       Running   0          20m


// ************** transition page **************************************************************************************
[#questions]
== Questions

[.noredheader,cols="65,.<45"]
|===

.2+|image:questions.png[width="95%",height="95%"]
a|* Twitter : *{speaker-twitter}*
|===







